## Performance Optimizations

### 1. Response Time Breakdown

**Target**: ≤ 5 seconds total

```
Total: 4520ms
├── Tavily API:        1500ms (33%)
├── Jina Reader:       2000ms (44%)
├── OpenAI Extract:    1000ms (22%)
└── Database:            20ms (0.4%)
```

### 2. Optimization Strategies

#### A. Parallel API Calls

**Before** (sequential):

```typescript
const search = await webSearch(query);      // 1500ms
const content = await fetchClean(url);      // 2000ms
Total: 3500ms
```

**After** (parallel):

```typescript
const [search, content] = await Promise.all([
  webSearch(query),      // 1500ms
  fetchClean(url)        // 2000ms
]);
Total: 2000ms (max of both)
```

#### B. Content Truncation

```typescript
// Limit to 12k characters
const trimmed = text.slice(0, 12000);
```

**Impact**:

- Reduces OpenAI token usage
- Faster processing
- Lower costs

#### C. Aggressive Caching

```typescript
// Cache search results (1 hour)
// Cache page content (24 hours)
```

**Impact**:

- 73% faster on cache hits
- Reduced API costs
- Better rate limit management

#### D. Streaming Responses

```typescript
// Start streaming before all tools complete
return streamText({ ... });
```

**Impact**:

- Better perceived performance
- User sees progress immediately
- Lower time-to-first-token

### 3. Database Optimization

#### A. Non-Blocking Inserts

```typescript
queueMicrotask(async () => {
  await supabase.from("products").insert(products);
});
```

**Impact**:

- Doesn't slow down response
- User gets results faster

#### B. Batch Inserts

```typescript
// Insert all products at once
await supabase.from("products").insert(products); // Array
```

**Impact**:

- Single round-trip vs multiple
- Faster than individual inserts

### 4. Bundle Size Optimization

**Next.js Automatic**:

- Code splitting
- Tree shaking
- Minification

**Manual**:

```typescript
// Use dynamic imports for heavy components
const HeavyComponent = dynamic(() => import("./Heavy"));
```

### 5. Edge Runtime Benefits

**Cold Start**:

- Node.js: ~500-1000ms
- Edge: ~50-100ms
- **10x faster cold starts**

**Geographic Distribution**:

- User in São Paulo → edge in São Paulo
- User in Tokyo → edge in Tokyo
- **Lower latency everywhere**

---
